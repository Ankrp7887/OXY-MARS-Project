{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bs4\n",
    "!pip install selenium\n",
    "!pip install webdriver_manager\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Variables containing perticular job details\n",
    "#'Enter Main Company link(Having 25 entries each) :\n",
    "complink = ['https://www.monsterindia.com/search/neo-sense-vector-technology-private-limited-503727-jobs-career?searchId=db84723c-51fd-4461-bbeb-b84afa442ef9']\n",
    "\n",
    "#'Enter .CSV filename. ie, short Company name :      \n",
    "filename = 'neosensevector.csv'    \n",
    "\n",
    "#'Enter given email address :                                                                                                                                      \n",
    "email_address = 'neosensevectortechnologyprivatelimitedjobsandcareers@sunisbrite.com'      \n",
    "\n",
    "#'Enter Company UserName you entered :                                                                                  \n",
    "username = 'neosense'                      \n",
    "\n",
    "#'Enter Company name :                                                                                                                      \n",
    "company_name = 'Neo Sense Vector Technology Private Limited'\n",
    "\n",
    "#'Check total number of jobs and enter here :\n",
    "total_jobs = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating total no. of pages need to scrap\n",
    "string = complink[0]\n",
    "loc = string.find('jobs-career')\n",
    "loc = loc + 11\n",
    "last_page = 1\n",
    "if total_jobs > 25:\n",
    "    if total_jobs % 25 == 0:\n",
    "        x = int(total_jobs//25)\n",
    "    else:\n",
    "        x = int(total_jobs//25)\n",
    "        x = x+1\n",
    "\n",
    "    for i in range(2, x+1):\n",
    "        page = complink[0][0:loc] + f'-{i}' + complink[0][loc:]\n",
    "        complink.append(page)\n",
    "        last_page = i\n",
    "#print(complink)\n",
    "#print(complink[0:last_page])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definiotions of all required functions\n",
    "\n",
    "# 1 of 2\n",
    "def find_state(city):\n",
    "    if city[0] == 'Mumbai' or city[0] == 'Pune' or city[0] == 'Navi Mumbai' or city[0] == 'Thar' or city[0] == 'Mumbai City' or city[0] == 'Aurangabad' or city[0] == 'Nagpur' or city[0] == 'Dhule' or city[0] == 'Nashik' or city[0] == 'Karad' or city[0] == 'Kolhapur' or city[0] == 'Wardha' or city[0] == 'Yavatmal':\n",
    "        state = 'Maharashtra'\n",
    "    elif city[0] == 'Noida' or city[0] == 'Lucknow' or city[0] == 'Ghaziabad' or city[0] == 'Mathura' or city[0] == 'Allahabad' or city[0] == 'Agra' or city[0] == 'Aligarh':\n",
    "        state = 'Uttar Pradesh'\n",
    "    elif city[0] == 'Kochi' or city[0] == 'Cochin' or city[0] == 'Ernakulam' or city[0] == 'Kozhikode' or city[0] == 'Thiruvananthapuram' or city[0] == 'Palakkad' or city[0] == 'Kannur' or city[0] == 'Kottayam':\n",
    "        state = 'Kerala'\n",
    "    elif city[0] == 'Bengaluru' or city[0] == 'Bangalore' or city[0] == 'Mysore' or city[0] == 'Hospet' or city[0] == 'Bidar' or city[0] == 'Karwar' or city[0] == 'Dharwad':\n",
    "        state = 'Karnataka'\n",
    "    elif city[0] == 'Chennai' or city[0] == 'Coimbatore' or city[0] == 'Kovilpatti' or city[0] == 'Hosur' or city[0] == 'Tiruvallur' or city[0] == 'Madurai' or city[0] == 'Salem' or city[0] == 'Pudukkottai' or city[0] == 'Kanchipuram' or city[0] == 'Cuddalore':\n",
    "        state = 'Tamil Nadu'\n",
    "    elif city[0] == 'Ahmedabad' or city[0] == 'Vadodara' or city[0] == 'Navsari' or city[0] == 'Bharuch' or city[0] == 'Valsad' or city[0] == 'Surat' or city[0] == 'Mehsana' or city[0] == 'Himatnagar' or city[0] == 'Anand' or city[0] == 'Bardoli':\n",
    "        state = 'Gujarat'\n",
    "    elif city[0] == 'Gurugram' or city[0] == 'Gurgaon' or city[0] == 'Hisar' or city[0] == 'Panipat' or city[0] == 'Rohtak' or city[0] == 'Bahadurgarh':\n",
    "        state = 'Haryana'\n",
    "    elif city[0] == 'Kolkata' or city[0] == 'Salt' or city[0] == 'Siliguri' or city[0] == 'Durgapur':\n",
    "        state = 'West Bengal'\n",
    "    elif city[0] == 'Hyderabad' or city[0] == 'Secunderabad' or city[0] == 'Nalgonda' or city[0] == 'Warangal':\n",
    "        state = 'Telangana'\n",
    "    elif city[0] == 'Udaipur' or city[0] == 'Jaipur' or city[0] == 'Kishangarh' or city[0] == 'Hindaun' or city[0] == 'Nagar' or city[0] == 'Nathdwara' or city[0] == 'Kotputli' or city[0] == 'Raisinghnagar' or city[0] == 'Anupgarh' or city[0] == 'Pokaran' or city[0] == 'Jodhpur' or city[0] == 'Barmer' or city[0] == 'Banswara':\n",
    "        state = 'Rajasthan'\n",
    "    elif city[0] == 'Panchkula' or city[0] == 'Chandigarh':\n",
    "        state = 'Chandigarh'\n",
    "    elif city[0] == 'Ludhiana' or city[0] == 'Amritsar' or city[0] == 'Rajpura' or city[0] == 'Gurdaspur' or city[0] == 'Patiala':\n",
    "        state = 'Punjab'\n",
    "    elif city[0] == 'Motihari' or city[0] == 'Muzaffarpur' or city[0] == 'Patna' or city[0] == 'Gaya' or city[0] == 'Arrah' or city[0] == 'Bhagalpur':\n",
    "        state = 'Bihar'\n",
    "    elif city[0] == 'Rourkela' or city[0] == 'Sambalpur' or city[0] == 'Bhubaneswar' or city[0] == 'Puri' or city[0] == 'Deogarh':\n",
    "        state = 'Odisha'\n",
    "    elif city[0] == 'Shillong':\n",
    "        state = 'Meghalaya'\n",
    "    elif city[0] == 'Jamshedpur' or city[0] == 'Ranchi':\n",
    "        state = 'Jharkhand'\n",
    "    elif city[0] == 'Bilaspur':\n",
    "        state = 'Chhattisgarh'    \n",
    "    elif city[0] == 'Dhubri' or city[0] == 'Guwahati':\n",
    "        state = 'Assam'\n",
    "    elif city[0] == 'Rewa' or city[0] == 'Gwalior' or city[0] == 'Satna' or city[0] == 'Bhopal' or city[0] == 'Hoshangabad' or city[0] == 'Indore' or city[0] == 'Chhindwara':\n",
    "        state = 'Madhya Pradesh'  \n",
    "    elif city[0] == 'Tirupati' or city[0] == 'Bhimavaran' or city[0] == 'Nellore' or city[0] == 'Kadapa' or city[0] == 'Chittoor' or city[0] == 'Rajahmundry' or city[0] == 'Ongole' or city[0] == 'Tanuku' or city[0] == 'Visakhapatnam':\n",
    "        state = 'Andhra Pradesh'   \n",
    "    elif city[0] == 'Solan':\n",
    "        state = 'Himachal Pradesh'\n",
    "    elif city[0] == 'Pondicherry':\n",
    "        state = 'Pondicherry'  \n",
    "    elif city[0] == 'Dehradun' or city[0] == 'Nainital':\n",
    "        state = 'Uttarakhand'             \n",
    "    else:\n",
    "        state = 'Delhi'\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "# 2 of 2\n",
    "def identify_job_sector(x):\n",
    "    if 'Human' in x or 'Recruitment' in x or 'Human Resources' in x or 'Admin' in x:\n",
    "        x = 'HR Jobs'\n",
    "    elif 'Digital Marketing' in x or 'Marketing & Communications' in x or 'Advertising' in x:\n",
    "        x = 'Marketing Jobs'  \n",
    "    elif 'Health' in x or 'Hospitals' in x or 'Healthcare' in x or 'Diagnostics' in x or 'Personal Care' in x:\n",
    "        x = 'Health Care Jobs'\n",
    "    elif 'Customer Service' in x:\n",
    "        x = 'Consultant Jobs' \n",
    "    elif 'Manufacturing' in x:\n",
    "        x = 'Sales'\n",
    "    elif 'Purchase' in x or 'Finance & Accounts' in x:\n",
    "        x = 'Accounting Jobs'\n",
    "    elif 'Application' in x:\n",
    "        x = 'Application Programming Jobs'\n",
    "    elif 'Product Management' in x:\n",
    "        x = 'Maintenance Jobs'\n",
    "    elif 'Oil & Gas' in x or 'Engineering - Electronics' in x or 'Engineering - Chemical' in x:\n",
    "        x = 'Engineering'\n",
    "    elif 'Telecom Software' in x:\n",
    "        x = 'Telecom Software Jobs'\n",
    "    elif 'Banking' in x:\n",
    "        x = 'Bank Jobs'\n",
    "    elif 'System' in x:\n",
    "        x = 'System Programming Jobs'\n",
    "    elif 'Site' in x:\n",
    "        x = 'Site Engineering Jobs'\n",
    "    elif 'Network' in x:\n",
    "        x = 'Network administrator Jobs'\n",
    "    elif 'Interior' in x:\n",
    "        x = 'Interior Design Jobs'\n",
    "    elif 'IT' in x or 'Information' in x:\n",
    "        x = 'IT Jobs'\n",
    "    elif 'Export' in x:\n",
    "        x = 'Export Import'\n",
    "    elif 'Graphic' in x:\n",
    "        x = 'Graphic Designer Jobs'\n",
    "    elif 'Hardwer' in x:\n",
    "        x = 'Hardwer & Networking Jobs'\n",
    "    elif 'BPO' in x:\n",
    "       x = 'BPO / Call Centre'\n",
    "    elif 'Business' in x:\n",
    "        x = 'Business Intelligence Jobs'\n",
    "    elif 'Client' in x:\n",
    "        x = 'Client Server Jobs'\n",
    "    elif 'Content' in x or 'Journalism' in x:\n",
    "        x = 'Content Writing'\n",
    "    elif 'Corporate' in x:\n",
    "        x = 'Corporate Planning Jobs'\n",
    "    elif 'Education' in x:\n",
    "        x = 'Education Training'   \n",
    "    elif 'Accounting' in x or 'Airline' in x or 'Automobile' in x or 'Bank' in x or 'Analytics' in x or 'Consultant' in x or 'DBA' in x or 'Ecommerce' in x or 'EDP' in x:\n",
    "        x = x\n",
    "    elif 'Film' in x or 'Hotel' in x or 'Legal' in x or 'Logistics' in x:\n",
    "        x = x\n",
    "    elif 'Mainframe' in x or 'Maintenance' in x or 'Marketing' in x or 'Middleware' in x or 'Mobile' in x or 'Packaging' in x or 'Pharma' in x or 'Secretary' in x:\n",
    "        x = x\n",
    "    elif 'Security' in x or 'Shipping' in x or 'Testing' in x or 'VLSI' in x or 'ERP' in x:\n",
    "        x = x\n",
    "    else:\n",
    "        x = 'IT Jobs'\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                          ***Freshly(from 1st page) Scraping Should be done from this cell....***\n",
    "\n",
    "\n",
    "#temp list to store scraped data\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                 ***Ranged Scraping Should be done from this cell, if you given range for scraping.\n",
    "#                                                        ie, you scraped some pages and now startig scraping from some page, other than 1st....***\n",
    "\n",
    "\n",
    "\n",
    "#Enter the Page where on which got error...\n",
    "#Defaultly 'scraping_start_page = 1' to start from your given page\n",
    "scraping_start_page = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data from Job URL\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "scraping_start_page = scraping_start_page - 1\n",
    "data_length = 0\n",
    "\n",
    "#Give range in complink[x:y] list for, how many pages(batch of 25 jobs) you want to scrap data from...\n",
    "for lnk in complink[scraping_start_page:last_page]:      #Defaultly - complink[scraping_start_page:last_page]\n",
    "    \n",
    "    driver = webdriver.Edge(EdgeChromiumDriverManager().install())                     #|If you want to work with chromedriver, simply comment this line\n",
    "    #driver = webdriver.Chrome(ChromeDriverManager().install())                        #|If you want to work with chromedriver, simply remove comment symbol(#) of this line\n",
    "    driver.minimize_window()\n",
    "\n",
    "    page = complink.index(lnk) + 1\n",
    "    print(f'Page - {page}')\n",
    "    monsterindia_responce_checker_out = 0\n",
    "    monsterindia_responce_checker_in = 0\n",
    "\n",
    "    for i in range(3):\n",
    "        try:\n",
    "            driver.get(lnk)\n",
    "        except WebDriverException:\n",
    "            print(f\"MonsterIndia Not Responding, Checked {i+1} time...(After 3rd time exicution will be trminated)\")\n",
    "            monsterindia_responce_checker_out = monsterindia_responce_checker_out + 1\n",
    "            time.sleep(1)\n",
    "        time.sleep(1)\n",
    "    time.sleep(2)\n",
    "    if monsterindia_responce_checker_out >= 3:\n",
    "        print('Exicution Trminated')\n",
    "        break\n",
    "\n",
    "    resp = driver.page_source\n",
    "    soup = BeautifulSoup(resp, 'html.parser')\n",
    "    link_list = soup.select('[class=\"job-tittle\"] h3 a')\n",
    "    \n",
    "    for link in link_list:\n",
    "        url = link.get('href')\n",
    "        try: \n",
    "            driver.get(url)\n",
    "        except WebDriverException:\n",
    "            print(\"MonsterIndia.com Not Responding. Terminating cell exicution...\")\n",
    "            monsterindia_responce_checker_in = monsterindia_responce_checker_in + 1\n",
    "            break\n",
    "\n",
    "        resp_info = driver.page_source\n",
    "        soup_info = BeautifulSoup(resp_info, 'html.parser')\n",
    "        job_details = soup_info.select('.job-details-wrapper')\n",
    "        \n",
    "        for info in job_details:\n",
    "            job_title = (info.select('h1')[0].getText()).strip()\n",
    "            location = (info.select('.loc.jd-loc a')[0].getText()).strip()\n",
    "            location = location.split(' / ')[0]\n",
    "            try:\n",
    "                if location == 'India':\n",
    "                    location = (info.select('.loc.jd-loc a')[1].getText()).strip()\n",
    "                    location = location.split(' / ')[0]\n",
    "            except IndexError:  \n",
    "                pass      \n",
    "            \n",
    "            CITY = location.replace(',', '')\n",
    "            city = location.split(',')\n",
    "            state = find_state(city)\n",
    "            experience = (info.select('.exp.col-xxs-12.col-sm-3.text-ellipsis span small')[0].getText()).strip().replace(' years', '').split('-')\n",
    "            try:\n",
    "                if experience[0] == 'Not Specified':\n",
    "                    years = 'Fresh'\n",
    "                elif experience[0] == 'Fresher':\n",
    "                    years = 'Fresh'\n",
    "                elif int(experience[0]) == 0 or int(experience[1]) == 0:\n",
    "                    years = 'Fresh'\n",
    "                elif int(experience[0]) == 0 and int(experience[1]) == 1:\n",
    "                    years = 'Less Than 1 Year'\n",
    "                elif int(experience[0]) >= 8:\n",
    "                    years = '8 Years +'\n",
    "                elif int(experience[0]) >= 2 and int(experience[1]) <= 7:\n",
    "                    years = experience[0] + ' Years'\n",
    "                else: years = 'Fresh'\n",
    "            except IndexError:\n",
    "                years = 'Fresh'\n",
    "                continue\n",
    "            \n",
    "            description = info.select('.job-description-content .jd-text')[0].getText().strip().replace('<br/>', '\\n').replace(':', ':\\n').replace('\\t', '').replace('*', '\\n*')\n",
    "            description += '\\nJob Details\\n'\n",
    "            job_sector = None\n",
    "            key_skills = ''\n",
    "            for count, details in enumerate(info.find_all('div', class_ ='job-detail-list')):\n",
    "                dt_heading = details.find('div', class_ = 'dt-heading').text.replace('\\t', '')\n",
    "                dt_content = details.find('div', class_ = 'dt-content').text.replace('\\t', '')\n",
    "                description += f'\\n{dt_heading}\\n{dt_content}'\n",
    "                #algorithm\n",
    "                try:\n",
    "                    if count == 2:\n",
    "                        function = details.find('div', class_ = 'dt-content').text.replace('\\t', '').split('/')\n",
    "                        if 'Other' in function or 'Others' in function:\n",
    "                            job_sector = 'IT Jobs'\n",
    "                        else:\n",
    "                            job_sector = function[0]\n",
    "                    job_sector=(job_sector.split(',')[0]).strip()\n",
    "                    if count == 4:\n",
    "                        description += '\\nKey Skills\\n'\n",
    "                        for index, skill in enumerate(details.find_all('span', class_ = 'round-card mb5 grey-link')):\n",
    "                            description += f' âž¼ {skill.text}\\n'\n",
    "                            if index > 4:\n",
    "                                break\n",
    "                            key_skills += skill.text + ', '\n",
    "                        continue\n",
    "                except AttributeError:\n",
    "                    continue\n",
    "            if key_skills == \"\":\n",
    "                key_skills= ' '\n",
    "            \n",
    "            employment = (info.select('#jobDets [class=\"card-panel\"] > div:nth-of-type(1) .dt-content')[0].getText()).strip()\n",
    "            #industry = (info.select('#jobDets [class=\"card-panel\"] > div:nth-child(2) .dt-content')[0].getText()).strip()\n",
    "            #function = (info.select('#jobDets [class=\"card-panel\"] > div:nth-child(3) .dt-content')[0].getText()).strip()\n",
    "            #skills = (info.select('#jobDets [class=\"card-panel\"] > div:nth-child(4) .dt-content')[0].getText()).strip()\n",
    "            min_salary, max_salary = '', ''\n",
    "            try:\n",
    "                salary = (info.select('.package')[0].getText()).strip()\n",
    "                salary = salary.replace(' ', '').split('-')\n",
    "                if salary[0] == 'NotSpecified':\n",
    "                    min_salary += salary[1]\n",
    "                else:\n",
    "                    min_salary += salary[0]\n",
    "                max_salary += salary[1]\n",
    "            except:\n",
    "                salary = ''\n",
    "            if min_salary == '':\n",
    "                min_salary= ' '\n",
    "            if max_salary == '':\n",
    "                max_salary= ' '\n",
    "            temp = {\n",
    "                'Job Title': job_title,\n",
    "                'Location': location,\n",
    "                'State': state,\n",
    "                'CITY': CITY,\n",
    "                'City': city[0],\n",
    "                'Experience': years,\n",
    "                'Description': description,\n",
    "                'Employment': employment,\n",
    "                'Job Sector':job_sector,\n",
    "                'Key Skills': key_skills,\n",
    "                #'Industry':industry,\n",
    "                #'Function':function,\n",
    "                #'Skills': skills,\n",
    "                'Min Salary': min_salary,\n",
    "                'Max Salary': max_salary\n",
    "            }\n",
    "            \n",
    "            data.append(temp)\n",
    "            data_length = len(data)\n",
    "            print(f\"{data_length} ====> {temp['Job Title']}\")\n",
    "    if monsterindia_responce_checker_in >= 1:\n",
    "        print(f'Start scraping from current page againg and after csv created just delete some duplicate jobs in range-{page*25} to last scraped job {data_length}')\n",
    "        break\n",
    "    print(f'All job on page-{page} scraped...\\n')\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating CSV file to store scrapped data\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(filename,index=False,header=True)\n",
    "data_list = pd.read_csv(filename).values.tolist()\n",
    "len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Posting Function defined\n",
    "def posting_failed(filename_failed, try_, posting_start, last_posting):\n",
    "\n",
    "    #Reading CSV file\n",
    "    data_list = pd.read_csv(filename_failed).values.tolist()\n",
    "\n",
    "    if posting_start <= 2:\n",
    "        posting_start = 0\n",
    "    elif posting_start > 2:\n",
    "        posting_start = posting_start - 2\n",
    "\n",
    "    if try_ != 0:\n",
    "        last_posting = len(data_list) + 1\n",
    "    last_posting = last_posting - 1\n",
    "\n",
    "\n",
    "    #Variables created for Un-posted Job record\n",
    "    post_no = 0\n",
    "    post_count = 0\n",
    "    post_record = {}\n",
    "    fail_count = 0\n",
    "    fail_countt = 0\n",
    "    fail_record = {}\n",
    "    fail_index = []\n",
    "\n",
    "    #Posting Jobs form CSV file\n",
    "    for i in range(posting_start, last_posting):            \n",
    "        warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "        another_options = webdriver.EdgeOptions()                                          #|If you want to work with chromedriver, simply comment this line\n",
    "        #another_options = webdriver.ChromeOptions()                                       #|If you want to work with chromedriver, simply remove comment symbol(#) of this line\n",
    "        another_options.add_argument('--start-maximized')\n",
    "        another_options.add_argument('--incognito')\n",
    "        another_options.add_argument('--disable-gpu')\n",
    "        another_options.add_argument('--disable-geolocation')\n",
    "        another_options.add_argument('--disable-extensions')\n",
    "        \n",
    "        another_driver = webdriver.Edge(EdgeChromiumDriverManager().install())             #|If you want to work with chromedriver, simply comment this line\n",
    "        #another_driver = webdriver.Chrome(ChromeDriverManager().install())                #|If you want to work with chromedriver, simply remove comment symbol(#) of this line\n",
    "        another_driver.minimize_window()\n",
    "        \n",
    "        try:\n",
    "            another_driver.get('https://oxymars.com/india/post-new-job')\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # job title\n",
    "            another_driver.find_element_by_id('ad-posting-title').send_keys(data_list[i][0])\n",
    "        except:\n",
    "            print(f\"Error Occured at job row-{i+2} in CSV due to OXY-MARS website isn't responding\")\n",
    "            fail_countt = -1\n",
    "            print(\"cell exicution terminated...\")\n",
    "            break\n",
    "\n",
    "        # job description\n",
    "        if 10000 < len(data_list[i][6]):\n",
    "            data_list[i][6] = data_list[i][6][:9950]\n",
    "            pos = data_list[i][6].rfind('.')\n",
    "            data_list[i][6] = data_list[i][6][:pos+1]\n",
    "        data_list[i][6] = data_list[i][6].replace('\t',' ')\n",
    "        another_driver.switch_to.frame(\"job_detail_ifr\") # we need to change to frame in order to see the html elements/attribute\n",
    "        another_driver.find_element_by_id('tinymce').send_keys(data_list[i][6]) # enter full description\n",
    "\n",
    "        another_driver.switch_to.default_content() # switch to main page \n",
    "\n",
    "        #email address\n",
    "        another_driver.find_element_by_name('reg_user_email').send_keys(email_address)\n",
    "        \n",
    "        #username   \n",
    "        another_driver.find_element_by_name('reg_user_uname').send_keys(username)\n",
    "        \n",
    "        #company name\n",
    "        another_driver.find_element_by_name('pt_user_organization').send_keys(company_name)\n",
    "\n",
    "        #job sector\n",
    "        sector = identify_job_sector(data_list[i][8])\n",
    "        another_driver.find_element_by_id('job-sector-selectized').send_keys(sector)\n",
    "        another_driver.find_element_by_id('job-sector-selectized').send_keys(Keys.ENTER)\n",
    "\n",
    "        #job type\n",
    "        another_driver.find_element_by_id('job-type-selectized').send_keys(data_list[i][7])\n",
    "        another_driver.find_element_by_id('job-type-selectized').send_keys(Keys.ENTER)\n",
    "    \n",
    "        # skills here\n",
    "        data_list[i][9] = data_list[i][9].replace('\t','')\n",
    "        another_driver.find_element_by_xpath('//*[@id=\"job-skills\"]/li/input').send_keys(data_list[i][9])\n",
    "\n",
    "        # salary\n",
    "        another_driver.find_element_by_name('job_salary').send_keys(data_list[i][10])\n",
    "        another_driver.find_element_by_name('job_max_salary').send_keys(data_list[i][10])\n",
    "\n",
    "        #other info\n",
    "        another_driver.find_element_by_xpath('//*[@id=\"job-posting-form\"]/div[2]/ul/li[1]/div/div/div[1]/input').send_keys('Others')\n",
    "        another_driver.find_element_by_xpath('//*[@id=\"job-posting-form\"]/div[2]/ul/li[1]/div/div/div[1]/input').send_keys(Keys.ENTER)\n",
    "        \n",
    "        # experiece\n",
    "        another_driver.find_element_by_xpath('//*[@id=\"job-posting-form\"]/div[2]/ul/li[2]/div/div/div[1]/input').send_keys(data_list[i][5])\n",
    "        another_driver.find_element_by_xpath('//*[@id=\"job-posting-form\"]/div[2]/ul/li[2]/div/div/div[1]/input').send_keys(Keys.ENTER)\n",
    "\n",
    "        #gender\n",
    "        another_driver.find_element_by_xpath('//*[@id=\"job-posting-form\"]/div[2]/ul/li[3]/div/div/div[1]/input').send_keys('Any')\n",
    "        another_driver.find_element_by_xpath('//*[@id=\"job-posting-form\"]/div[2]/ul/li[3]/div/div/div[1]/input').send_keys(Keys.ENTER)\n",
    "        \n",
    "        #qualifications \n",
    "        another_driver.find_element_by_xpath('//*[@id=\"job-posting-form\"]/div[2]/ul/li[4]/div/div/div[1]/input').send_keys(\"Degree Bachelor\")\n",
    "        another_driver.find_element_by_xpath('//*[@id=\"job-posting-form\"]/div[2]/ul/li[4]/div/div/div[1]/input').send_keys(Keys.ENTER)\n",
    "\n",
    "        try:\n",
    "            #country   \n",
    "            another_driver.find_element_by_xpath('//*[@id=\"countryId-selectized\"]').send_keys('India')\n",
    "            another_driver.find_element_by_xpath('//*[@id=\"countryId-selectized\"]').send_keys(Keys.ENTER)\n",
    "            time.sleep(14)\n",
    "            \n",
    "            #state    \n",
    "            another_driver.find_element_by_xpath('//*[@id=\"stateId-selectized\"]').send_keys(data_list[i][2]) \n",
    "            another_driver.find_element_by_xpath('//*[@id=\"stateId-selectized\"]').send_keys(Keys.ENTER)\n",
    "            time.sleep(7)\n",
    "\n",
    "            #city\n",
    "            if data_list[i][2] == 'Delhi':\n",
    "                pass\n",
    "            elif data_list[i][2] == 'Orrisa':\n",
    "                pass\n",
    "            elif data_list[i][2] == 'Jammu Kashmir':\n",
    "                pass\n",
    "            else:    \n",
    "                another_driver.find_element_by_xpath('//*[@id=\"cityId-selectized\"]').send_keys(data_list[i][4])\n",
    "                another_driver.find_element_by_xpath('//*[@id=\"cityId-selectized\"]').send_keys(Keys.ENTER)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        CITY=data_list[i][3]\n",
    "        complete_address = f'{CITY}, India'\n",
    "        if 'India' in CITY:\n",
    "            complete_address = CITY\n",
    "        another_driver.find_element_by_name('jobsearch_field_location_address').send_keys(complete_address)\n",
    "\n",
    "        try:\n",
    "            another_driver.find_element_by_name('terms_cond_check').click()\n",
    "            time.sleep(2)\n",
    "            another_driver.find_element_by_css_selector('input[type=\"submit\"]').click()\n",
    "        except:\n",
    "            pass\n",
    "            print(f\"Error Occured at job row-{i+2} in CSV due to OXY-MARS website isn't responding\")\n",
    "            fail_countt = -1\n",
    "            print(\"cell exicution terminated...\")\n",
    "            break\n",
    "\n",
    "        time.sleep(7)\n",
    "\n",
    "        try:\n",
    "            soup = BeautifulSoup(another_driver.page_source, 'html.parser')\n",
    "            confirmation = (soup.find('div', class_ = 'jobsearch-employer-confitmation').h2).text.replace(' ','')\n",
    "            if confirmation == 'Thankyouforsubmitting':\n",
    "                post_no = post_no + 1\n",
    "                print(f'{post_no}.Posting job of row no.{i+2} from CSV file done...')\n",
    "                post_count = post_count+1\n",
    "                post_record[i+2] = data_list[i][0]\n",
    "            else:\n",
    "                print(f'Job of row-{i+2} {data_list[i][0]} in CSV file is not posted. Just wait, posting next job********in*******\\n')\n",
    "                fail_index.append(i)\n",
    "                fail_record[i+2] = data_list[i][0]\n",
    "                fail_count = fail_count+1   \n",
    "        except AttributeError:\n",
    "            print(f'\\nJob of row-{i+2} {data_list[i][0]} in CSV file is not posted. Just wait, posting next job********out********\\n')\n",
    "            fail_index.append(i)\n",
    "            fail_record[i+2] = data_list[i][0]\n",
    "            fail_count = fail_count+1\n",
    "\n",
    "        another_driver.quit()\n",
    "\n",
    "    print(f'\\nTotal No of Jobs Posted - {post_count}')\n",
    "    print('list of Jobs Successfully Posted', post_record)\n",
    "    print(f'Total No of Jobs Failed to Post - {fail_count}')\n",
    "    print('list of Jobs Failed to get Posted', fail_record)\n",
    "    data_list_for_failed = pd.read_csv(filename_failed)\n",
    "\n",
    "    failed_jobs = []\n",
    "    for j in fail_index:\n",
    "        failed_jobs_dict = data_list_for_failed.iloc[j].to_dict()\n",
    "        failed_jobs.append(failed_jobs_dict)\n",
    "    if fail_count > 0: \n",
    "        fd = pd.DataFrame(failed_jobs)\n",
    "        filename_failed = 'failed_jobs_' + str(try_) + '.csv'\n",
    "        fd.to_csv(filename_failed,index=False,header=True)\n",
    "    elif fail_countt == -1:\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Congratulations, you did posted all jobs from MonsterIndia for your Given Task/Company\")\n",
    "\n",
    "    posting_start = 2\n",
    "    return fail_count, filename_failed, fail_countt, posting_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                          ***Posting function is getting call from this cell....***\n",
    "\n",
    "\n",
    "filename_failed = filename\n",
    "dlst = pd.read_csv(filename_failed).values.tolist()\n",
    "last_posting = len(dlst) + 1\n",
    "#Do not change lines above\n",
    "\n",
    "            \n",
    "posting_start = 2               #Enter row no. to start posting from that row of CSV file(defaultly give 2, as first job is at row-2 in CSV)\n",
    "#last_posting = 10              #If you want to give end range for posting, remove '#' symbol on starting of this line & replace 10 to certain number jobs...\n",
    "                                    #give number like till which job/row(including) in csv you want to post in one exicution.\n",
    "\n",
    "\n",
    "#Calling Function\n",
    "for i in range(0, 10):\n",
    "\n",
    "    p = posting_failed(filename_failed, i, posting_start, last_posting)\n",
    "    filename_failed = p[1]\n",
    "    posting_start = p[3]\n",
    "    if p[0] == 0:\n",
    "        break\n",
    "    elif p[2] == -1:\n",
    "        break\n",
    "    elif i == 9:\n",
    "        print('We tried 10 times, if you seeing this msg. You need to contact Group/Team leader')\n",
    "        break\n",
    "    print(f\"\\n\\nTrying {i+2}th time Again for all failed jobs\\n\\n\")\n",
    "    time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fab145c62319e31a41c9a6790a7fad7c720880be9303f3e3ea8a2856d3d97e4d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
